{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"/figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<font size=\"4\"><center><b>Disciplina: Novas Tecnologias em Banco de Dados</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Profa. Dra. SAHUDY MONTENEGRO GONZÁLEZ</center></font>\n",
    "\n",
    "## <center>Projeto Final</center>\n",
    "\n",
    "*INTEGRANTES*\n",
    "\n",
    "**Integrante 01**: Laura Rieko Marçal Imai\n",
    "\n",
    "**RA**: \n",
    "\n",
    "**Integrante 02**: Pedro Enrico Barchi Nogueira\n",
    "\n",
    "**RA**: 813099\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração e Integração de Dados de Publicações Químicas via Web Scraping\n",
    "\n",
    "## Sumário\n",
    "1. [Introdução](#introducao)\n",
    "2. [Inicialização e Preparação do Ambiente](#inicializacao-e-preparacao-do-ambiente)\n",
    "3. [Extração dos Dados Básicos (articles.csv)](#extracao-dos-dados-basicos)\n",
    "4. [Adição da métrica Total Access](#adicao-total-access)\n",
    "5. [Análise Exploratória](#analise-exploratoria)\n",
    "6. [Tratamento dos Dados para o Data Warehouse](#tratamento-dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"introducao\"></a>Introdução\n",
    "\n",
    "Neste projeto, desenvolvemos um pipeline de web scraping para extrair e integrar dados de publicações científicas de duas importantes revistas químicas: **Química Nova (QN)** e **Journal of the Brazilian Chemical Society (JBCS)**. O objetivo é coletar informações básicas dos artigos (como título, autores, data de publicação, etc.) e, posteriormente, complementar estes dados com a métrica \"Total Access\" diretamente dos sites das revistas no portal SBQ. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"inicializacao-e-preparacao-do-ambiente\"></a>Inicialização e Preparação do Ambiente\n",
    "\n",
    "Nesta seção, preparamos o ambiente de trabalho importando as bibliotecas necessárias e configurando os scripts que compõem o pipeline do projeto. O projeto foi desenvolvido em **Python 3.12.4** e utiliza módulos como `requests` e `BeautifulSoup` para o scraping, além de ferramentas para manipulação e visualização dos dados. A estrutura do projeto está organizada em scripts separados, permitindo que as funções principais (para extração dos dados básicos e para o complemento do 'Total Access') sejam chamadas de forma modular a partir deste notebook.\n",
    "\n",
    "### Instalando Pacotes Necessários\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pacotes para manipulação de dados e web scraping\n",
    "%pip install requests beautifulsoup4 pandas numpy matplotlib seaborn unidecode rapidfuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importações e Configuração Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "# Configuração do logger para acompanhamento do fluxo\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"ScraperPublicaçõesQuímicas\")\n",
    "\n",
    "# Importação dos scripts do projeto\n",
    "\n",
    "from scrapers.scraper_basico import run_scraper\n",
    "\n",
    "from scrapers.scraper_total_access import run_total_access\n",
    "\n",
    "from tratamento.tratamento_dados import tratar_dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"extracao-dos-dados-basicos\"></a>Extração dos Dados Básicos (articles.csv)\n",
    "\n",
    "Nesta etapa, utilizamos o módulo `scraper_basico.py` para extrair as informações fundamentais de publicações das revistas Química Nova (QN) e Journal of the Brazilian Chemical Society (JBCS) a partir da SciELO.  \n",
    "O resultado deste processo é armazenado em um arquivo CSV intermediário chamado **articles.csv**.  \n",
    "Esta etapa realiza a extração dos dados básicos, como título, autores, data de publicação, entre outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa o scraper para extrair os artigos\n",
    "logger.info(\"Iniciando a extração dos dados básicos (articles.csv)...\")\n",
    "artigos = run_scraper()  # run_scraper() retorna uma lista de dicionários com os dados dos artigos\n",
    "\n",
    "logger.info(f\"Total de artigos extraídos: {len(artigos)}\")\n",
    "\n",
    "# Visualiza os 3 primeiros artigos para verificação\n",
    "for idx, art in enumerate(artigos[:3], start=1):\n",
    "    logger.info(f\"Artigo {idx}: {art}\")\n",
    "\n",
    "# Salva os dados extraídos no arquivo 'articles.csv'\n",
    "output_csv_path = \"articles.csv\"\n",
    "fieldnames = [\n",
    "    \"journal\",\n",
    "    \"year\",\n",
    "    \"volume\",\n",
    "    \"edition_number\",\n",
    "    \"publication_date\",\n",
    "    \"publication_type\",\n",
    "    \"title\",\n",
    "    \"authors\",\n",
    "    \"keywords\",\n",
    "    \"institutions\"\n",
    "]\n",
    "\n",
    "logger.info(f\"Salvando os dados extraídos em {output_csv_path}...\")\n",
    "with open(output_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for art in artigos:\n",
    "        # Caso os campos como authors, keywords, institutions estejam como lista, converta-os em string\n",
    "        art[\"authors\"] = \"; \".join(art[\"authors\"]) if isinstance(art[\"authors\"], list) else art[\"authors\"]\n",
    "        art[\"keywords\"] = \"; \".join(art[\"keywords\"]) if isinstance(art[\"keywords\"], list) else art[\"keywords\"]\n",
    "        art[\"institutions\"] = \"; \".join(art[\"institutions\"]) if isinstance(art[\"institutions\"], list) else art[\"institutions\"]\n",
    "        if isinstance(art.get(\"publication_date\"), (datetime,)):\n",
    "            art[\"publication_date\"] = art[\"publication_date\"].strftime(\"%Y-%m-%d\")\n",
    "        writer.writerow(art)\n",
    "logger.info(f\"Extração concluída! {output_csv_path} foi criado com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"adicao-total-access\"></a>Adição da métrica Total Access\n",
    "\n",
    "Agora que já extraímos as informações básicas das publicações e armazenamos no arquivo `articles.csv`, iremos complementar esses dados com a métrica **Total Access**.\n",
    "\n",
    "O **Total Access** representa o número de acessos de cada artigo e será obtido diretamente dos sites das revistas **Química Nova (QN)** e **Journal of the Brazilian Chemical Society (JBCS)**. Para isso, utilizamos o módulo `scraper_total_access.py`, que busca as edições correspondentes de cada periódico e extrai as informações de acesso para cada artigo.\n",
    "\n",
    "Ao final dessa etapa, um novo arquivo CSV atualizado será gerado: **articles_final.csv**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define os caminhos dos arquivos de entrada e saída\n",
    "input_csv = \"articles.csv\"\n",
    "output_csv = \"articles_final.csv\"\n",
    "\n",
    "print(\"Iniciando o processo de compleção do 'Total Access'...\")\n",
    "run_total_access(input_csv, output_csv)\n",
    "print(f\"Processo concluído! Arquivo gerado: {output_csv}\")\n",
    "\n",
    "# Exibir os primeiros registros do CSV final para verificação\n",
    "import pandas as pd\n",
    "df_final = pd.read_csv(output_csv)\n",
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"analise-exploratoria\"></a>Análise Exploratória dos Dados\n",
    "\n",
    "Nesta etapa, vamos realizar uma análise exploratória básica do arquivo **articles_final.csv** que contém os dados integrados das publicações com a métrica **TotalAccess**. Com isso, temos o objetivo de verificar a integridade dos dados e identificar registros com valores nulos em **TotalAccess**, exibir estatísticas descritivas da métrica de acesso e visualizar a distribuição dos acessos com gráficos.\n",
    "\n",
    "Em seguida, removeremos os registros com valor nulo em **TotalAccess** para o tratamento posterior dos dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset final\n",
    "df = pd.read_csv(\"articles_final.csv\")\n",
    "print(\"Número de registros originais:\", len(df))\n",
    "\n",
    "# Converter a coluna 'TotalAccess' para numérico; registros não convertíveis serão definidos como NaN\n",
    "df['TotalAccess'] = pd.to_numeric(df['TotalAccess'], errors='coerce')\n",
    "\n",
    "# Verificar a quantidade de registros com TotalAccess nulo\n",
    "missing_total = df['TotalAccess'].isnull().sum()\n",
    "print(\"Registros com TotalAccess nulo:\", missing_total)\n",
    "\n",
    "# Exibir uma amostra dos registros com TotalAccess nulo (se houver)\n",
    "if missing_total > 0:\n",
    "    print(\"Exemplo de registros com TotalAccess nulo:\")\n",
    "    display(df[df['TotalAccess'].isnull()].head())\n",
    "\n",
    "# Remover registros onde TotalAccess é nulo\n",
    "df_clean = df.dropna(subset=['TotalAccess'])\n",
    "print(\"Número de registros após remover nulos:\", len(df_clean))\n",
    "\n",
    "# Exibir estatísticas descritivas da coluna TotalAccess\n",
    "print(\"\\nEstatísticas descritivas de TotalAccess:\")\n",
    "print(df_clean['TotalAccess'].describe())\n",
    "\n",
    "# Histograma da distribuição de TotalAccess\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_clean['TotalAccess'], bins=30, kde=True, color='skyblue')\n",
    "plt.title(\"Distribuição do TotalAccess\")\n",
    "plt.xlabel(\"TotalAccess\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"tratamento-dados\"></a> Tratamento dos Dados para o Data Warehouse\n",
    "\n",
    "Nesta etapa, aplicamos operações de limpeza e transformação para padronizar e preparar os dados das publicações para inserí-los no Data Warehouse. As operações incluem:\n",
    "\n",
    "- Padronização dos nomes das instituições (remoção de acentos, conversão para minúsculas e eliminação de duplicatas);\n",
    "- Normalização das palavras-chave, convertendo-as para minúsculas e removendo acentos;\n",
    "- Mapeia as palavras-chave para subáreas da química e adiciona a coluna \"subareas\";\n",
    "- Separação e padronização dos nomes dos autores;\n",
    "- Conversão das datas para um formato consistente (YYYY-MM-DD) e extração dos componentes ano, mês e dia;\n",
    "- Criação de um identificador único para cada edição, concatenando volume e número;\n",
    "- Remoção de registros duplicados (mesmo título) entre as revistas.\n",
    "\n",
    "Após o processamento, os dados serão salvos em um novo arquivo CSV que poderá ser carregado no DW.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados tratados e salvos em articles_dw.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define os caminhos para o CSV de entrada e saída\n",
    "input_csv = \"articles_final.csv\"\n",
    "output_csv = \"articles_dw.csv\"\n",
    "\n",
    "# Executa o processo de tratamento\n",
    "tratar_dados(input_csv, output_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
